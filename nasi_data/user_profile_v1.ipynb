{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec56dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T07:07:40.410345Z",
     "start_time": "2024-02-18T07:07:39.645587Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import json, os\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import operator\n",
    "\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "flags = [False]\n",
    "task = \"user_profile_v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c09c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T07:07:40.415301Z",
     "start_time": "2024-02-18T07:07:40.412318Z"
    }
   },
   "outputs": [],
   "source": [
    "contexts = ['display', 'cid', 'bundle', 'pub', 'subcat', 'adid', 'advertiser_id', 'tagid', 'size', 'crid',\n",
    "            'make', 'state', 'adtype', 'month', 'camera_pixels', 'osv', 'reward', 'appcat', 'os', 'chipset',\n",
    "            'devicetype', 'city', 'model', 'hour']\n",
    "\n",
    "contexts = [\"osv\", \"advertiser_id\", \"bundle\", \"cid\", \"crid\"]\n",
    "base_path = \"hdfs://nameservice1/user/hive/warehouse/lixiang.db/%s\" % task\n",
    "codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da2cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T07:07:40.423956Z",
     "start_time": "2024-02-18T07:07:40.416897Z"
    }
   },
   "outputs": [],
   "source": [
    "def profile_cnt(xs, day):\n",
    "    data = pd.DataFrame(map(lambda x: x.asDict(), xs))\n",
    "    ans = {\"cnt\": len(data), \"time\": int(day.timestamp())}\n",
    "    for k in contexts:\n",
    "        ans.update({k: data.groupby(k).size().to_dict()})\n",
    "    return ans\n",
    "\n",
    "\n",
    "def filter_uid(x, day):\n",
    "    if x[\"time\"] > 0:\n",
    "        if (day - datetime.datetime.fromtimestamp(x[\"time\"])).days > 90:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65603c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T07:07:40.216Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.now() - datetime.timedelta(days=30)\n",
    "now = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "dates = pd.date_range(start_date, now)\n",
    "for i in range(2, len(dates)):\n",
    "    last_day = dates[i - 1]\n",
    "    day = dates[i]\n",
    "    y, m, d = day.strftime(\"%Y\"), day.strftime(\"%m\"), day.strftime(\"%d\")\n",
    "    \n",
    "    cmd = \"hadoop fs -test -e %s/daily/%s/_SUCCESS;echo $?\" % (base_path, day.strftime(\"%Y%m%d\"))\n",
    "    out = exec_cmd(cmd)\n",
    "    print(\"cmd>>>\", cmd, out, day.strftime(\"%Y%m%d\"))\n",
    "    if out is not None and int(out) != 0:\n",
    "        spark = set_spark(task, flags)\n",
    "        sc = spark.sparkContext\n",
    "        \n",
    "        paths = []\n",
    "        for evt in [\"imp\", \"clk\"]:\n",
    "            with open(\"./%s.sql\" % evt) as f:\n",
    "                sql = f.read()\n",
    "            sql = sql.format(y=y, m=m, d=d)\n",
    "            \n",
    "            df = spark.sql(sql)\n",
    "            for k, dtype in dict(df.dtypes).items():\n",
    "                if dtype == \"string\":\n",
    "                    df = df.withColumn(k, F.udf(fill_na)(k))\n",
    "            rdd = df.rdd.map(lambda x: (x[\"deviceid\"], [x])).reduceByKey(operator.add) \\\n",
    "                        .mapValues(lambda xs: json.dumps({evt: profile_cnt(xs, day)})) \\\n",
    "                        .repartition(50) \\\n",
    "                        .map(lambda x: \"%s\\t\\t%s\" % (x[0], x[1]))\n",
    "            savedPath = \"%s/%s/%s\" % (base_path, evt, day.strftime(\"%Y%m%d\"))\n",
    "            paths.append(savedPath)\n",
    "            rdd.saveAsTextFile(savedPath, codec)\n",
    "        #merge imp/clk\n",
    "        rdd = sc.textFile(\",\".join(paths)).map(lambda x: x.strip(\"\\n\").split(\"\\t\\t\")).mapValues(lambda x: [json.loads(x)])\n",
    "        rdd = rdd.reduceByKey(operator.add) \\\n",
    "            .mapValues(lambda lst: json.dumps(map_union(lst))) \\\n",
    "            .repartition(50) \\\n",
    "            .map(lambda x: \"%s\\t\\t%s\" % (x[0], x[1]))\n",
    "        savedPath = \"%s/daily/%s\" % (base_path, day.strftime(\"%Y%m%d\"))\n",
    "        # savedPath = \"%s/%s\" % (base_path, day.strftime(\"%Y%m%d\"))\n",
    "        rdd.saveAsTextFile(savedPath, codec)\n",
    "        \n",
    "        # break\n",
    "        # merge last day\n",
    "        last_profile = \"%s/%s\" % (base_path, last_day.strftime(\"%Y%m%d\"))\n",
    "        profile = \"%s/daily/%s\" % (base_path, day.strftime(\"%Y%m%d\"))\n",
    "        rdd1 = sc.textFile(last_profile).map(lambda x: x.strip(\"\\n\").split(\"\\t\\t\")).mapValues(lambda x: [weight_decay(json.loads(x))])\n",
    "        rdd2 = sc.textFile(profile).map(lambda x: x.strip(\"\\n\").split(\"\\t\\t\")).mapValues(lambda x: [json.loads(x)])\n",
    "\n",
    "        yd_profile = rdd1.union(rdd2).reduceByKey(operator.add) \\\n",
    "                .mapValues(lambda lst: map_reduce(lst)) \\\n",
    "                .filter(lambda x: filter_uid(x[1], day)) \\\n",
    "                .mapValues(json.dumps) \\\n",
    "                .repartition(50) \\\n",
    "                .map(lambda x: \"%s\\t\\t%s\" % (x[0], x[1]))\n",
    "\n",
    "        savedPath = \"%s/%s\" % (base_path, day.strftime(\"%Y%m%d\"))\n",
    "        yd_profile.saveAsTextFile(savedPath, codec)\n",
    "\n",
    "        ##rmr\n",
    "        for evt in [\"imp\", \"clk\"]:\n",
    "            cmd = \"hadoop fs -rmr  %s/%s/%s\" % (base_path, evt, day.strftime(\"%Y%m%d\"))\n",
    "            exec_cmd(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe3b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104725c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5109997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7913e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f500c8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T06:44:08.243Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e9de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d591b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3c6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcc11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b9c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9106484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d440837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a37f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
